@article{10.1145/3591302,
author = {Wilkinson, Lucas and Cheshmi, Kazem and Dehnavi, Maryam Mehri},
title = {Register Tiling for Unstructured Sparsity in Neural Network Inference},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591302},
doi = {10.1145/3591302},
abstract = {Unstructured sparse neural networks are an important class of machine learning (ML) models, as they compact model size and reduce floating point operations. The execution time of these models is frequently dominated by the sparse matrix multiplication (SpMM) kernel, C=A\texttimes{} B, where A is a sparse matrix, and B and C are dense matrices. The unstructured sparsity pattern of matrices in pruned machine learning models along with their sparsity ratio has rendered useless the large class of libraries and systems that optimize sparse matrix multiplications. Reusing registers is particularly difficult because accesses to memory locations should be known statically. This paper proposes Sparse Register Tiling, a new technique composed of an unroll-and-sparse-jam transformation followed by data compression that is specifically tailored to sparsity patterns in ML matrices. Unroll-and-sparse-jam uses sparsity information to jam the code while improving register reuse. Sparse register tiling is evaluated across 2396 weight matrices from transformer and convolutional models with a sparsity range of 60-95\% and provides an average speedup of 1.72\texttimes{} and 2.65\texttimes{} over MKL SpMM and dense matrix multiplication, respectively, on a multicore CPU processor. It also provides an end-to-end speedup of 2.12\texttimes{} for MobileNetV1 with 70\% sparsity on an ARM processor commonly used in edge devices.},
journal = {Proc. ACM Program. Lang.},
month = {jun},
articleno = {188},
numpages = {26},
keywords = {Sparse Matrix, Pruned Neural Networks, Loop Tiling}
}